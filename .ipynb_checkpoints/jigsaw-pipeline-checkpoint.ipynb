{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPAS Analysis with DivDis\n",
    "\n",
    "This notebook implements the DivDis (Diverse Disagreement) approach on the COMPAS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'captum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_base_dfs, COMPASDataset, create_data_splits, explain_model, class_weights\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DivDisClassifier, BaseClassifier, ModelWrapper, HeadWrapper, ResidualMLPClassifier\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_model, trainDivDis\n",
      "File \u001b[0;32m~/code/unbiased-classifiers/utils.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, random_split, Dataset\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexplainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Explainer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelWrapper\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m~/code/unbiased-classifiers/explainer.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LimeBase, Lime, ShapleyValueSampling, FeaturePermutation, GradientShap, IntegratedGradients\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SkLearnLinearModel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_exp_kernel_similarity_function\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'captum'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from utils import get_base_dfs, COMPASDataset, create_data_splits, explain_model, class_weights\n",
    "from models import DivDisClassifier, BaseClassifier, ModelWrapper, HeadWrapper, ResidualMLPClassifier\n",
    "from train import train_model, trainDivDis\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from explainer import Explainer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1999516, 46)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1083994</td>\n",
       "      <td>He got his money... now he lies in wait till a...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-03-06 15:21:53.675241+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317120</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>650904</td>\n",
       "      <td>Mad dog will surely put the liberals in mental...</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-12-02 16:44:21.329535+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154086</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5902188</td>\n",
       "      <td>And Trump continues his lifelong cowardice by ...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-09-05 19:05:32.341360+00</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>374342</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7084460</td>\n",
       "      <td>\"while arresting a man for resisting arrest\".\\...</td>\n",
       "      <td>test</td>\n",
       "      <td>2016-11-01 16:53:33.561631+00</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149218</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5410943</td>\n",
       "      <td>Tucker and Paul are both total bad ass mofo's.</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-06-14 05:08:21.997315+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344096</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text  split  \\\n",
       "0  1083994  He got his money... now he lies in wait till a...  train   \n",
       "1   650904  Mad dog will surely put the liberals in mental...  train   \n",
       "2  5902188  And Trump continues his lifelong cowardice by ...  train   \n",
       "3  7084460  \"while arresting a man for resisting arrest\".\\...   test   \n",
       "4  5410943     Tucker and Paul are both total bad ass mofo's.  train   \n",
       "\n",
       "                    created_date  publication_id  parent_id  article_id  \\\n",
       "0  2017-03-06 15:21:53.675241+00              21        NaN      317120   \n",
       "1  2016-12-02 16:44:21.329535+00              21        NaN      154086   \n",
       "2  2017-09-05 19:05:32.341360+00              55        NaN      374342   \n",
       "3  2016-11-01 16:53:33.561631+00              13        NaN      149218   \n",
       "4  2017-06-14 05:08:21.997315+00              21        NaN      344096   \n",
       "\n",
       "     rating  funny  wow  ...  white  asian  latino  other_race_or_ethnicity  \\\n",
       "0  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "1  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "2  approved      1    0  ...    NaN    NaN     NaN                      NaN   \n",
       "3  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "4  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "\n",
       "   physical_disability  intellectual_or_learning_disability  \\\n",
       "0                  NaN                                  NaN   \n",
       "1                  NaN                                  NaN   \n",
       "2                  NaN                                  NaN   \n",
       "3                  NaN                                  NaN   \n",
       "4                  NaN                                  NaN   \n",
       "\n",
       "   psychiatric_or_mental_illness  other_disability  identity_annotator_count  \\\n",
       "0                            NaN               NaN                         0   \n",
       "1                            NaN               NaN                         0   \n",
       "2                            NaN               NaN                         0   \n",
       "3                            NaN               NaN                         0   \n",
       "4                            NaN               NaN                         0   \n",
       "\n",
       "   toxicity_annotator_count  \n",
       "0                        67  \n",
       "1                        76  \n",
       "2                        63  \n",
       "3                        76  \n",
       "4                        80  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns:\n",
      "['id', 'comment_text', 'split', 'created_date', 'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes', 'disagree', 'toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat', 'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability', 'identity_annotator_count', 'toxicity_annotator_count']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_jigsaw\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Check NaN values in race-related columns\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m race_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(race \u001b[38;5;129;01min\u001b[39;00m col\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m race \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124masian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatino\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhispanic\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Display percentage of NaN values for each race column\u001b[39;00m\n\u001b[1;32m     15\u001b[0m nan_stats \u001b[38;5;241m=\u001b[39m df[race_columns]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_jigsaw = pd.read_csv('jigsaw_dataset/all_data.csv')\n",
    "\n",
    "# Display first few rows and basic info\n",
    "print(\"Dataset shape:\", df_jigsaw.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df_jigsaw.head())\n",
    "print(\"\\nColumns:\")\n",
    "print(df_jigsaw.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NaN values in race-related columns\n",
    "race_columns = [col for col in df_jigsaw.columns if any(race in col.lower() for race in ['race', 'black', 'white', 'asian', 'latino', 'hispanic'])]\n",
    "\n",
    "# Display percentage of NaN values for each race column\n",
    "nan_stats = df_jigsaw[race_columns].isna().mean() * 100\n",
    "\n",
    "print(\"Percentage of NaN values in race columns:\")\n",
    "for col, pct in nan_stats.items():\n",
    "    print(f\"{col}: {pct:.2f}%\")\n",
    "\n",
    "# Display total number of rows\n",
    "print(f\"\\nTotal rows in dataset: {len(df_jigsaw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Load and prepare the COMPAS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "non_violent, violent = get_base_dfs()\n",
    "\n",
    "# Define features\n",
    "train_columns = [\n",
    "    \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\",\n",
    "    \"priors_count\", \"african-american\", \"caucasian\", \"hispanic\",\n",
    "    \"other\", \"asian\", \"native-american\", \"less25\", \"greater45\",\n",
    "    \"25to45\", \"felony\", \"misdemeanor\", \"two_years_r\"\n",
    "]\n",
    "\n",
    "# Create dataset\n",
    "dataDf = violent[train_columns]\n",
    "data = COMPASDataset(dataDf, \"two_years_r\")\n",
    "\n",
    "# Split data\n",
    "trainData, unlabelData, _, testData, rawTrain, rawUnlabel, _, testRaw = create_data_splits(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model Training\n",
    "Train a baseline ResidualMLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input dimensions\n",
    "dataiter = iter(trainData)\n",
    "batch = next(dataiter)\n",
    "train_columns.pop()\n",
    "inputs, targets = batch\n",
    "input_dim = int(inputs.shape[1])\n",
    "\n",
    "# Initialize base model\n",
    "baseModel = ResidualMLPClassifier(\n",
    "    input_dim=input_dim,  \n",
    "    hidden_dim=64,        \n",
    "    num_blocks=1,\n",
    "    dropout_rate=0.3,\n",
    "    num_classes=2         \n",
    ")\n",
    "\n",
    "# Prepare data and train\n",
    "fullTrain = ConcatDataset([rawTrain, rawUnlabel])\n",
    "fullLoader = DataLoader(fullTrain, 64, True)\n",
    "weights = class_weights(fullTrain)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Train base model\n",
    "baseModel, history = train_model(baseModel, fullLoader, testData, criterion, 3, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DivDis Model Training\n",
    "Train the DivDis model with multiple heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train DivDis model\n",
    "divDisModel = DivDisClassifier(input_dim)\n",
    "full_loss = trainDivDis(divDisModel, 15, trainData, unlabelData, testData, criterion)\n",
    "\n",
    "# Explain predictions for each head\n",
    "for head in range(divDisModel.num_heads):\n",
    "    model_name = f\"DivDis model head_{head}\"\n",
    "    model_head = HeadWrapper(divDisModel, head)\n",
    "    model_head.eval()\n",
    "    explain_model(model_head, divDisModel.num_classes, testRaw, model_name, input_dim, train_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
